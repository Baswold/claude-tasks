# Gesture Sync Studio

**Audio-to-Gesture Blender Plugin & Training Framework**

Generate realistic sitting/gesturing animations for rigged characters from audio input using rule-based generation or machine learning models.

## Features

- ðŸŽ¤ **Audio-driven animation**: Generate natural gestures synchronized with speech
- ðŸ¤– **Dual generation modes**: Rule-based (no training needed) or ML-powered
- ðŸŽ¨ **Blender integration**: Easy-to-use addon with GUI panel
- ðŸ“Š **Training framework**: Complete PyTorch-based training pipeline
- ðŸ”§ **Configurable**: Extensive customization options
- âš¡ **Real-time capable**: Process 10 minutes of audio in under 1 minute

## Project Structure

```
gesture-sync-studio/
â”œâ”€â”€ blender_addon/          # Blender addon for gesture generation
â”‚   â”œâ”€â”€ __init__.py         # Main addon file
â”‚   â”œâ”€â”€ audio_processor.py  # Audio feature extraction
â”‚   â”œâ”€â”€ gesture_generator.py # Gesture generation (rule-based + ML)
â”‚   â”œâ”€â”€ animation_applier.py # Apply gestures to armature
â”‚   â”œâ”€â”€ ui_panel.py         # Blender UI
â”‚   â”œâ”€â”€ operators.py        # Blender operators
â”‚   â””â”€â”€ gesture_config.json # Configuration file
â”œâ”€â”€ training/               # ML training framework
â”‚   â”œâ”€â”€ data_loader.py      # Dataset loader
â”‚   â”œâ”€â”€ model.py            # Neural network models
â”‚   â”œâ”€â”€ train.py            # Training script
â”‚   â”œâ”€â”€ export.py           # Model export for Blender
â”‚   â””â”€â”€ training_config.yaml # Training configuration
â”œâ”€â”€ dataset/                # Training data
â”‚   â”œâ”€â”€ audio/              # Audio files
â”‚   â”œâ”€â”€ motion/             # Motion data (JSON/BVH)
â”‚   â””â”€â”€ metadata.json       # Dataset metadata
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ tests/                  # Test suite
â””â”€â”€ examples/               # Example files
```

## Quick Start

### Installation

#### 1. Install Blender Addon

1. Download or clone this repository
2. Open Blender (3.6+ or 4.0+)
3. Go to Edit > Preferences > Add-ons
4. Click "Install" and select `blender_addon` folder
5. Enable "Audio Gesture Generator" addon
6. Install dependencies when prompted (or manually via pip)

#### 2. Install Dependencies

The addon requires Python packages that may not be included with Blender:

```bash
# Using Blender's Python
/path/to/blender/python -m pip install numpy librosa scipy

# Or use the built-in dependency installer in the addon panel
```

### Using the Addon

1. **Select Audio File**: Choose a WAV, MP3, or OGG audio file
2. **Select Armature**: Choose the rigged character to animate
3. **Adjust Parameters**:
   - FPS: Animation framerate (default: 24)
   - Gesture Intensity: How pronounced gestures should be
   - Smoothing: Amount of motion smoothing
4. **Generate**: Click "Generate Gesture Animation"

The addon will:
- Analyze the audio file
- Extract features (energy, pitch, emphasis points)
- Generate bone animations
- Apply keyframes to your armature

### Expected Bone Names

The addon expects standard bone naming:
- `head`, `neck`
- `spine`, `spine.001`, `spine.002`
- `shoulder.L`, `shoulder.R`
- `upper_arm.L`, `upper_arm.R`
- `forearm.L`, `forearm.R`
- `hand.L`, `hand.R`

You can customize bone mapping in `gesture_config.json`.

## Training Your Own Model

### Prerequisites

```bash
pip install torch torchvision torchaudio
pip install librosa numpy scipy pyyaml tqdm tensorboard
```

### Prepare Dataset

Organize your data as:

```
dataset/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ clip_001.wav
â”‚   â”œâ”€â”€ clip_002.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ motion/
â”‚   â”œâ”€â”€ clip_001.json
â”‚   â”œâ”€â”€ clip_002.json
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json
```

**Motion Data Format** (JSON):

```json
{
  "fps": 24,
  "bones": {
    "head": {
      "frames": [
        {
          "time": 0.0,
          "rotation": [1, 0, 0, 0],  // Quaternion (w, x, y, z)
          "location": [0, 0, 1.6]     // XYZ position
        }
      ]
    }
  }
}
```

### Train Model

```bash
cd training

# Train with default config
python train.py --dataset ../dataset --output ./output

# Train with custom config
python train.py --dataset ../dataset --config my_config.yaml --output ./output

# Resume from checkpoint
python train.py --dataset ../dataset --output ./output --resume ./output/checkpoint_epoch_50.pt
```

### Export Model for Blender

```bash
# Export to ONNX (recommended)
python export.py --checkpoint ./output/best_model.pt --output gesture_model.onnx --format onnx

# Export to TorchScript
python export.py --checkpoint ./output/best_model.pt --output gesture_model.pt --format torchscript
```

Copy the exported model to `blender_addon/models/` and select it in the addon UI.

## Configuration

### Addon Configuration

Edit `blender_addon/gesture_config.json`:

```json
{
  "bone_mapping": {
    "head": "Head",
    "neck": "Neck"
  },
  "animation_settings": {
    "fps": 24,
    "gesture_intensity": 1.0,
    "smoothing": 0.5
  },
  "audio_processing": {
    "sample_rate": 22050,
    "n_mfcc": 13
  }
}
```

### Training Configuration

Edit `training/training_config.yaml`:

```yaml
model:
  type: "lstm"  # or "simple", "transformer"
  hidden_dim: 256
  num_layers: 3

training:
  batch_size: 16
  learning_rate: 0.001
  num_epochs: 100
```

## How It Works

### Rule-Based Generation

The rule-based system maps audio features to bone movements:

1. **Audio Analysis**: Extract RMS energy, zero-crossing rate, spectral features
2. **Feature Mapping**:
   - High energy â†’ larger gestures
   - Pauses â†’ return to idle pose
   - Emphasis points â†’ accent gestures (head nods, hand movements)
3. **Idle Motion**: Continuous subtle breathing and idle animations
4. **Smoothing**: Temporal smoothing to reduce jitter

### ML-Based Generation

The ML model learns from training data:

1. **Feature Extraction**: MFCC + energy + spectral features from audio
2. **Sequence Model**: LSTM/Transformer maps audio features to bone parameters
3. **Loss Function**: MSE + temporal smoothness + velocity matching
4. **Inference**: Generate frame-by-frame bone rotations and positions

## API Documentation

See [docs/API.md](docs/API.md) for detailed API documentation.

## Troubleshooting

### Addon doesn't appear in Blender
- Make sure you installed the `blender_addon` folder, not individual files
- Check Blender version is 3.6+
- Look for errors in Blender's console (Window > Toggle System Console)

### "librosa not available" error
- Install dependencies: `/path/to/blender/python -m pip install librosa`
- Or use the dependency installer in the addon panel

### Gestures look unnatural
- Increase smoothing parameter
- Adjust gesture intensity
- Check that bone names match expected format
- Try different audio files (clear speech works best)

### Training fails
- Check dataset format matches specification
- Reduce batch size if running out of memory
- Ensure audio and motion files are paired correctly

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## License

MIT License - see LICENSE file for details

## Acknowledgments

- Built with PyTorch and Librosa
- Inspired by audio-driven animation research
- Blender community for addon development resources

## Citation

If you use this project in research, please cite:

```bibtex
@software{gesture_sync_studio,
  title = {Gesture Sync Studio: Audio-to-Gesture Animation System},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/gesture-sync-studio}
}
```

## Support

- Issues: [GitHub Issues](https://github.com/yourusername/gesture-sync-studio/issues)
- Discussions: [GitHub Discussions](https://github.com/yourusername/gesture-sync-studio/discussions)
- Email: your.email@example.com
